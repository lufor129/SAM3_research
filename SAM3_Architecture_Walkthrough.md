# SAM3 Architecture Walkthrough

> **Paper-to-Code Mapping** for the SAM3 Research Project

## Architecture Overview (from Paper Figure 10)

The SAM3 architecture extends SAM2 with new components for **multimodal detection and tracking**. The diagram shows three component origins:
- ğŸŸ¡ **Yellow**: New components in SAM3
- ğŸ”µ **Blue**: Components from SAM2
- ğŸŸ¢ **Cyan**: Components from Perception Encoder (PE)

```mermaid
flowchart LR
    subgraph Inputs
        text[Text]
        geometry[Geometry]
        image[Image]
    end
    
    subgraph Backbone["Backbone (SAM3VLBackbone)"]
        TextEncoder[Text Encoder ğŸŸ¢]
        ImageEncoder[Image Encoder ğŸŸ¢]
    end
    
    subgraph Detection["Detection Pipeline"]
        Detector[Detector ğŸŸ¡]
        MemoryAttention[Memory Attention ğŸ”µ]
        MaskletMatcher[Masklet Matcher ğŸŸ¡]
        Tracker[Tracker ğŸ”µ]
        MemoryBank[Memory Bank ğŸ”µ]
    end
    
    subgraph Decoder["Multimodal Decoder"]
        ExemplarEncoder[Exemplar Encoder ğŸŸ¡]
        MultimodalDecoder[Multimodal Decoder ğŸŸ¡]
        DetectorDecoder[Detector Decoder ğŸŸ¡]
        PixelDecoder[Pixel Decoder ğŸ”µ]
        SemanticSegHead[Semantic Seg Head ğŸŸ¡]
        Heads[Heads ğŸŸ¡]
    end
    
    text --> TextEncoder
    geometry --> Detector
    image --> ImageEncoder
    
    TextEncoder --> Detector
    ImageEncoder --> Detector
    ImageEncoder --> MemoryAttention
    
    Detector --> MaskletMatcher
    MemoryAttention --> Tracker
    MaskletMatcher --> MemoryBank
    Tracker --> MemoryBank
    MemoryBank --> MaskletMatcher
    MemoryBank --> Tracker
    
    ExemplarEncoder --> MultimodalDecoder
    MultimodalDecoder --> DetectorDecoder
    MultimodalDecoder --> PixelDecoder
    DetectorDecoder --> Heads
    PixelDecoder --> SemanticSegHead
    
    Heads --> output[mask, boxes, scores]
```

---

## Core Component Code Mapping

### 1. Image Encoder (ViT Backbone)
**Paper**: Vision Transformer backbone for visual feature extraction  
**Code**: [vitdet.py](file:///e:/Research/SAM3_research/sam3/model/vitdet.py)

| Class | Purpose | Key Features |
|-------|---------|--------------|
| `ViT` (L568-882) | Main ViT backbone | Patch embedding, windowed attention, RoPE |
| `PatchEmbed` (L301-338) | Image â†’ patches | Conv2d projection |
| `Attention` (L341-517) | Multi-head attention | Relative pos, 2D-RoPE |
| `Block` | Transformer block | Self-attention + FFN |

```python
# Key functions in vitdet.py
apply_rotary_enc()      # L70-92: Apply 2D rotary embeddings
window_partition()      # L95-115: Partition for windowed attention
window_unpartition()    # L118-141: Reverse window partition
```

---

### 2. Text Encoder
**Paper**: Language backbone for text understanding  
**Code**: [text_encoder_ve.py](file:///e:/Research/SAM3_research/sam3/model/text_encoder_ve.py)

| Class | Lines | Purpose |
|-------|-------|---------|
| `VETextEncoder` | L255-330 | Main text encoder with tokenizer |
| `TextTransformer` | L166-252 | CLIP-style text transformer |
| `Transformer` | L92-147 | Stack of attention blocks |
| `ResidualAttentionBlock` | L15-89 | Self-attention + cross-attention |

---

### 3. Vision-Language Backbone Combiner
**Paper**: Combines vision and language without fusion  
**Code**: [vl_combiner.py](file:///e:/Research/SAM3_research/sam3/model/vl_combiner.py)

```python
class SAM3VLBackbone(nn.Module):   # L18-177
    # Combines: Sam3DualViTDetNeck (visual) + VETextEncoder (text)
    
    def forward(self, samples, captions, input_boxes, additional_text):
        output = self.forward_image(samples)      # Vision features
        output.update(self.forward_text(...))     # Language features
        return output
```

**Output Dictionary**:
- `vision_features`: Image features from backbone
- `language_features`: Text features from encoder
- `language_mask`: Attention mask for text
- `vision_pos_enc`: Positional encodings
- `backbone_fpn`: Feature pyramid outputs

---

### 4. Geometry Encoder (Prompt Encoding)
**Paper**: Encodes geometric prompts (boxes, points, masks)  
**Code**: [geometry_encoders.py](file:///e:/Research/SAM3_research/sam3/model/geometry_encoders.py)

| Class | Lines | Purpose |
|-------|-------|---------|
| `Prompt` | L83-401 | Utility class for geometric prompts |
| `SequenceGeometryEncoder` | L470-838 | Full geometry encoder |
| `MaskEncoder` | L404-422 | Base mask encoder |
| `FusedMaskEncoder` | L425-467 | Mask + pixel feature fusion |

```python
class Prompt:
    # Manages: box_embeddings, point_embeddings, mask_embeddings
    # Methods: append_boxes(), append_points(), append_masks()
    
class SequenceGeometryEncoder:
    # Encodes boxes as points (optionally)
    # ROI pooling for box features
    # Position encoding for geometry
```

---

### 5. Transformer Encoder
**Paper**: Fuses text and image features  
**Code**: [encoder.py](file:///e:/Research/SAM3_research/sam3/model/encoder.py)

| Class | Lines | Purpose |
|-------|-------|---------|
| `TransformerEncoderLayer` | L15-251 | Self-attn â†’ Cross-attn |
| `TransformerEncoder` | L254-461 | Multi-level feature encoder |
| `TransformerEncoderFusion` | L464-579 | **Text-Image fusion encoder** |

```python
class TransformerEncoderFusion(TransformerEncoder):
    # Key: Fuses text and image features
    # add_pooled_text_to_img_feat: Add pooled text to image features
    # pool_text_with_mask: Use mask for text pooling
```

---

### 6. Transformer Decoder
**Paper**: Detector Decoder with presence token  
**Code**: [decoder.py](file:///e:/Research/SAM3_research/sam3/model/decoder.py)

| Class | Lines | Purpose |
|-------|-------|---------|
| `TransformerDecoderLayer` | L28-184 | Self-attn â†’ Cross-attn â†’ FFN |
| `TransformerDecoder` | L187-608 | Main detector decoder |
| `TransformerEncoderCrossAttention` | L611-720 | Cross-attention encoder variant |

```python
class TransformerDecoder:
    # num_queries: Detection query tokens
    # presence_head: Predicts object presence ğŸŸ¡
    # box_head: Predicts bounding boxes
    # use_text_cross_attention: Text-guided detection
```

---

### 7. Segmentation Head
**Paper**: Pixel Decoder + Semantic Segmentation Head  
**Code**: [maskformer_segmentation.py](file:///e:/Research/SAM3_research/sam3/model/maskformer_segmentation.py)

| Class | Lines | Purpose |
|-------|-------|---------|
| `PixelDecoder` | L174-221 | Upsamples features for masks |
| `MaskPredictor` | L25-53 | Generates mask predictions |
| `SegmentationHead` | L56-171 | Instance segmentation |
| `UniversalSegmentationHead` | L224-327 | Semantic + Instance seg |
| `LinearPresenceHead` | L16-22 | Presence prediction |

---

### 8. Memory Bank & Tracker
**Paper**: Per-object memory bank for video tracking  
**Code**: 
- [memory.py](file:///e:/Research/SAM3_research/sam3/model/memory.py) - Memory encoder
- [sam3_tracker_base.py](file:///e:/Research/SAM3_research/sam3/model/sam3_tracker_base.py) - Tracker

```python
class SimpleMaskEncoder:  # memory.py L160-203
    # Encodes masks for memory bank
    # Components: mask_downsampler, fuser, position_encoding
    
class SimpleMaskDownSampler:  # L21-80
    # Progressive mask downsampling
```

---

### 9. Main SAM3 Image Model
**Paper**: Complete SAM3 pipeline  
**Code**: [sam3_image.py](file:///e:/Research/SAM3_research/sam3/model/sam3_image.py)

```python
class Sam3Image(nn.Module):  # L33-681
    TEXT_ID_FOR_TEXT = 0
    TEXT_ID_FOR_VISUAL = 1  
    TEXT_ID_FOR_GEOMETRIC = 2
    
    def __init__(self, backbone, transformer, input_geometry_encoder, 
                 segmentation_head, dot_prod_scoring, ...):
        # Combines all components
    
    # Core forward methods:
    def _get_img_feats()      # L114-164: Get image features
    def _encode_prompt()      # L166-209: Encode text/visual/geometric prompts
    def _run_encoder()        # L211-249: Run transformer encoder
    def _run_decoder()        # L251-297: Run transformer decoder
    def _run_segmentation_heads()  # L385-423: Generate masks
    def forward_grounding()   # L439-490: Full grounding pipeline
```

---

### 10. Model Builder
**Code**: [model_builder.py](file:///e:/Research/SAM3_research/sam3/model_builder.py)

Key builder functions that assemble the architecture:

| Function | Lines | Purpose |
|----------|-------|---------|
| `build_sam3_image_model` | L560-641 | **Main image model builder** |
| `build_sam3_video_model` | L653-791 | Video model builder |
| `_create_vit_backbone` | L72-99 | ViT backbone |
| `_create_vl_backbone` | L113-115 | Vision-Language backbone |
| `_create_transformer_encoder` | L118-153 | Encoder creation |
| `_create_transformer_decoder` | L156-190 | Decoder creation |
| `_create_geometry_encoder` | L235-288 | Geometry encoder |
| `_create_segmentation_head` | L207-232 | Segmentation head |

---

## Data Flow Summary

```mermaid
sequenceDiagram
    participant I as Image
    participant T as Text
    participant G as Geometry
    participant B as Backbone
    participant E as Encoder
    participant D as Decoder
    participant S as SegHead
    participant O as Output
    
    I->>B: forward_image()
    T->>B: forward_text()
    G->>E: encode_prompt()
    B->>E: vision_features + language_features
    E->>D: encoder_hidden_states
    D->>S: obj_queries
    S->>O: masks, boxes, scores
```

---

## Key Innovations (from Paper)

1. **Presence Token** ğŸŸ¡ - New token to predict object presence
2. **Exemplar Encoder** ğŸŸ¡ - Encodes visual examples for few-shot learning
3. **Multimodal Decoder** ğŸŸ¡ - Handles text, visual, and geometric prompts
4. **Masklet Matcher** ğŸŸ¡ - Matches masklets across frames
5. **Semantic Seg Head** ğŸŸ¡ - Unified semantic + instance segmentation

---

## File Structure Summary

```
sam3/model/
â”œâ”€â”€ sam3_image.py          # Main SAM3 image model (Sam3Image)
â”œâ”€â”€ sam3_video_*.py        # Video tracking variants
â”œâ”€â”€ model_builder.py       # Model construction functions
â”œâ”€â”€ vitdet.py              # ViT backbone (Image Encoder)
â”œâ”€â”€ text_encoder_ve.py     # Text Encoder
â”œâ”€â”€ vl_combiner.py         # Vision-Language Backbone
â”œâ”€â”€ geometry_encoders.py   # Prompt encoding (box/point/mask)
â”œâ”€â”€ encoder.py             # Transformer Encoder
â”œâ”€â”€ decoder.py             # Transformer Decoder
â”œâ”€â”€ maskformer_segmentation.py  # Segmentation heads
â”œâ”€â”€ memory.py              # Memory bank components
â””â”€â”€ box_ops.py             # Box coordinate utilities
```

---

## Cross-Image Few-Shot Detection

> Reference: `examples/cross_image_few_shot_detection.ipynb`

### Q1: `geo_feats` æ˜¯ Detector çš„ output å—ï¼Ÿ

**å¦ï¼** `geo_feats` æ˜¯ **Geometry Encoder çš„è¼¸å‡º**ï¼Œæ˜¯ Detector çš„ **è¼¸å…¥** ä¹‹ä¸€ã€‚

```mermaid
flowchart LR
    A[Support Image + BBox] --> B[Image Encoder]
    B --> C[img_feats]
    D[BBox coordinates] --> E[Geometry Encoder]
    C --> E
    E --> F["geo_feats<br/>(Visual Prompt)"]
    F --> G[Transformer Encoder]
    G --> H[Transformer Decoder]
    H --> I[Detector Output<br/>boxes, masks, scores]
```

åœ¨ `extract_visual_prompt_embedding` ä¸­ï¼š
```python
# 1. å–å¾— Support Image çš„è¦–è¦ºç‰¹å¾µ
img_feats = processor.model._get_img_feats(backbone_out, ...)

# 2. Geometry Encoder ç·¨ç¢¼ BBox â†’ geo_feats (é Detector è¼¸å‡ºï¼)
geo_feats, geo_masks = processor.model.geometry_encoder(
    geo_prompt=geometric_prompt,    # BBox åº§æ¨™
    img_feats=img_feats,            # åœ–åƒç‰¹å¾µ (ç”¨æ–¼ ROI pooling)
    img_pos_embeds=img_pos_embeds,  # ä½ç½®ç·¨ç¢¼
)
```

**`geo_feats` çµ„æˆ**ï¼š
- BBox åº§æ¨™çš„ç›´æ¥æŠ•å½± (`boxes_direct_project`)
- å¾ `img_feats` é€²è¡Œ ROI pooling çš„å€åŸŸç‰¹å¾µ (`boxes_pool`)
- ä½ç½®ç·¨ç¢¼ (`boxes_pos_enc`)

---

### Q2: Support Image èˆ‡ Query Image å¦‚ä½•é€²è¡Œ Few-Shotï¼Ÿ

æ ¸å¿ƒæ©Ÿåˆ¶ï¼š**å°‡ Support Image çš„ Visual Prompt æ³¨å…¥åˆ° Query Image çš„æ¨è«–éç¨‹**

```mermaid
sequenceDiagram
    participant S as Support Image
    participant Q as Query Image
    participant GE as Geometry Encoder
    participant PE as _encode_prompt
    participant E as Transformer Encoder
    participant D as Transformer Decoder
    
    Note over S: Step 1: Extract Visual Prompt
    S->>GE: Image + BBox
    GE->>GE: ROI pooling + pos encoding
    GE-->>S: geo_feats (visual_prompt_embed)
    
    Note over Q: Step 2: Inject & Detect
    Q->>PE: Query features + visual_prompt_embed
    PE->>PE: Concatenate prompts
    PE->>E: Combined prompt
    E->>D: Encoder hidden states
    D-->>Q: Detected boxes, masks, scores
```

#### é—œéµç¨‹å¼ç¢¼ (grounding_with_visual_prompt):

```python
# Step 1: å¾ Support Image æå– visual prompt
visual_prompt_embed, visual_prompt_mask = extract_visual_prompt_embedding(
    processor, support_image, support_box
)

# Step 2: æ³¨å…¥åˆ° Query Image çš„ _encode_prompt
prompt, prompt_mask, backbone_out = processor.model._encode_prompt(
    backbone_out,           # Query Image çš„ backbone è¼¸å‡º
    find_input,
    geometric_prompt,
    visual_prompt_embed=visual_prompt_embed,  # ğŸ‘ˆ Support ç‰¹å¾µæ³¨å…¥
    visual_prompt_mask=visual_prompt_mask
)

# Step 3-5: Encoder â†’ Decoder â†’ Segmentation
backbone_out, encoder_out, _ = processor.model._run_encoder(...)
out, hs = processor.model._run_decoder(...)
processor.model._run_segmentation_heads(...)
```

#### æµç¨‹å°ç…§è¡¨ï¼š

| éšæ®µ | Support Image | Query Image |
|------|---------------|-------------|
| 1. Backbone | âœ… æå–è¦–è¦ºç‰¹å¾µ | âœ… æå–è¦–è¦ºç‰¹å¾µ |
| 2. Geometry Encoder | âœ… ç”¨ BBox æå– `geo_feats` | âŒ ä¸éœ€è¦ BBox |
| 3. Prompt çµ„åˆ | - | âœ… æ³¨å…¥ Support çš„ `geo_feats` |
| 4. Encoder â†’ Decoder | - | âœ… ä½¿ç”¨çµ„åˆå¾Œçš„ prompt |
| 5. è¼¸å‡º | - | âœ… boxes, masks, scores |

**æœ¬è³ª**ï¼šç”¨ Support Image çš„ BBox å€åŸŸç‰¹å¾µä½œç‚ºã€Œè¦–è¦ºæç¤ºã€ï¼Œå‘Šè¨´æ¨¡å‹ã€Œåœ¨ Query Image ä¸­æ‰¾é¡ä¼¼é€™å€‹å€åŸŸçš„ç‰©é«”ã€ã€‚
